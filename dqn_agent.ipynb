{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#強化学習 脳みそマン作成\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "class DQNagent():\n",
    "    \"\"\"クラスラベルはself.n_actions\"\"\"\n",
    "    def __init__(self,enable_actions,environment_name,hidden_layer_size,rows,cols):\n",
    "        self.environment_name=environment_name\n",
    "        self.enable_actions=list(enable_actions)\n",
    "        self.hidden_layer_size=30\n",
    "        self.n_actions=len(self.enable_actions)\n",
    "        self.rows=rows\n",
    "        self.cols=cols\n",
    "        \n",
    "        #minibatch learning\n",
    "        self.batch_size=32\n",
    "        #学習回数\n",
    "        self.replay_memory_size=1000\n",
    "        #学習率\n",
    "        self.learning_rate=0.01\n",
    "        #割引率\n",
    "        self.discount_factor=0.9\n",
    "        #exploration\n",
    "        self.exploration=0.1\n",
    "        \n",
    "        #遷移D\n",
    "        self.D=deque(maxlen=self.replay_memory_size)\n",
    "        \n",
    "        #モデルの初期化\n",
    "        self.init_model()\n",
    "        #損失関数\n",
    "        self.current_loss=0.0\n",
    "        \n",
    "    def init_model(self):\n",
    "        #二次元\n",
    "        self.x=tf.placeholder(tf.float32,shape=[None,self.rows,self.cols])\n",
    "        #教師ラベル\n",
    "        self.y_=tf.placeholder(tf.float32,[None,self.n_actions])\n",
    "        #ニューラルネットワークを実装する際には一次元変換\n",
    "        x_flat=tf.reshape(self.x,[-1,self.rows*self.cols])\n",
    "        \n",
    "        #全結合層\n",
    "        size=self.rows*self.cols\n",
    "        #第一層\n",
    "        w_fc=tf.Variable(tf.truncated_normal([size,size],stddev=0.01))\n",
    "        b_fc=tf.Variable(tf.zeros([size]))\n",
    "        h_fc=tf.nn.relu(tf.matmul(x_flat,w_fc)+b_fc)\n",
    "        \n",
    "        #第二層\n",
    "        w_fc2=tf.Variable(tf.truncated_normal([size,self.hidden_layer_size],stddev=0.01))\n",
    "        b_fc2=tf.Variable(tf.zeros([self.hidden_layer_size]))\n",
    "        h_fc2=tf.nn.relu(tf.matmul(h_fc,w_fc2)+b_fc2)\n",
    "        \n",
    "        #出力層\n",
    "        w_out=tf.Variable(tf.truncated_normal([self.hidden_layer_size,n_actions],stddev=0.01))\n",
    "        b_out=tf.Variable(tf.zeros([self.n_actions]))\n",
    "        self.y=tf.matmul(h_fc2,w_out)+b_out\n",
    "        \n",
    "        #損失関数\n",
    "        self.loss=tf.reduce_mean(tf.suqare(self.y_-self.y))\n",
    "        \n",
    "        #最適化指標\n",
    "        optimizer=tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.optimizer=optimizer.minimize(self.loss)\n",
    "        \n",
    "        #保存インスタンス\n",
    "        self.saver=tf.train.Saver()\n",
    "        \n",
    "        #session\n",
    "        self.sess=tf.Session()\n",
    "        self.sess.run(tf.global_varibles_initializer())\n",
    "        \n",
    "        \n",
    "        \n",
    "    def Q_vals(self,state):\n",
    "        \"\"\"state:現在の盤面\n",
    "        　　各ステップ終了後の盤面のこと\"\"\"\n",
    "        feed={self.x:[state]}\n",
    "        return self.sess.run(self.y,feed_dict=feed)[0]\n",
    "    \n",
    "    def epilon_greedy(self,state,epsilon,targets):\n",
    "        \"\"\"targetsは行動\n",
    "           epsilon_greedy()は行動を返す関数\"\"\"\n",
    "        if np.random.rand()<epsilon:\n",
    "            return np.random.choice(targets)\n",
    "        else:\n",
    "            #Qを最大にするような行動をとる\n",
    "            qvalue,action=self.next_able_action(state,targets)\n",
    "            return action\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def next_able_action(self,state,targets):\n",
    "        Q=self.Q_vals(state)\n",
    "        \n",
    "        index=np.argsort(Q)\n",
    "        \n",
    "        for action in reversed(index):\n",
    "            if action in targets:\n",
    "                break\n",
    "                \n",
    "        qvalue=Q[action]\n",
    "        \n",
    "        return qvalue,action\n",
    "    \n",
    "    #経験を学んでいく関数\n",
    "    #遷移Dに状態、行動、報酬、終了判定を保存\n",
    "    def store_experience(self,state,targets,action,reward,state_1,targets_1,terminal):\n",
    "        \"\"\"terminalは終了判定\"\"\"\n",
    "        self.D.append(state,targets,action,reward,state_1,targets_1,terminal)\n",
    "        \n",
    "    #経験を再現して学習する関数\n",
    "    #遷移Dからミニバッチ的に取り出してくる\n",
    "    def experience_replay(self):\n",
    "        #minibatch learningの準備\n",
    "        state_minibatch=[]\n",
    "        y_minibatch=[]\n",
    "        \n",
    "        #Dのstateの数がミニバッチよりも小さかった時は\n",
    "        minibatch_size=min(len(self.D),self.batch_size)\n",
    "        indexes=np.random.randint(0,len(self.D),minibatch_size)\n",
    "        \n",
    "        for j in indexes:\n",
    "            #遷移Dから取り出す\n",
    "            state_j,targets_j,action_j,reward_j,state_j_1,targets_j_1,terminal=self.D[j]\n",
    "            #行動にインデックスをつける\n",
    "            action_j_index=self.enable_actions.index(action_j)\n",
    "            #価値関数、教師信号の初期化\n",
    "            y_j=self.Q_vals(state_j)\n",
    "            \n",
    "            if terminal:\n",
    "                y_j[action_j]=reward_j\n",
    "            else:\n",
    "                #next_able_action()でQが最大化した値と行動がとってこれる\n",
    "                qvalue,action=self.next_able_action(state_j_1,targets_j_1)\n",
    "                y_j[action_j]=reward_j+self.discount_factor*qvalue\n",
    "                \n",
    "                \n",
    "            state_minibatch.append(state_j)\n",
    "            y_minibatch.append(y_j)\n",
    "            \n",
    "            \n",
    "            \n",
    "        #training\n",
    "        \"\"\"入力データはstate_minibatch\n",
    "           教師データはy_minibatch\"\"\"\n",
    "        self.sess.run(self.optimizer,feed_dict={self.x:state_minibatch,self.y_:y_minibatch})\n",
    "        \n",
    "        #log\n",
    "        self.current_loss=self.sess.run(self.loss,feed_dict={self.x:state_minibatch,self.y_:y_minibatch})\n",
    "        \n",
    "        \n",
    "    #load,saveする関数\n",
    "    def save_model(self,epoch,path='./osero-model/'):\n",
    "        if not os.path.isdir(path):\n",
    "            os.mkdir(path)\n",
    "            \n",
    "        self.saver.save(self.sess,os.path.join(path,'model.ckpt'),global_step=epoch)\n",
    "        \n",
    "    def load_model(self,epoch,path=None):\n",
    "        if path:\n",
    "            self.saver.restore(self.sess,path)\n",
    "        else:\n",
    "            self.saver.restore(self.sess,os.path.join(path,'model.ckpt-%d'%epoch))\n",
    "                \n",
    "                \n",
    "    \n",
    "        \n",
    "        \n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
